# Follow The Goat - Project Rules

## Project Overview
This project is a clean rewrite migrating features from `000old_code/` to a new architecture.
Once migration is complete, `000old_code/` will be deleted.

---

## ğŸš¨ CRITICAL: DUAL-MASTER ARCHITECTURE ğŸš¨

**THIS SYSTEM USES TWO SEPARATE PROCESSES WITH DIFFERENT DATABASES**

### Master.py - DATA ENGINE (Port 5050)
**Purpose:** Raw data ingestion only - runs indefinitely without restarts

**Database Setup:**
1. **TradingDataEngine** - In-memory DuckDB (primary hot storage)
   - Runs continuously with zero file locks
   - Stores: prices, order book, trades (hot data only)
   - Exposed via FastAPI on port 5050
   
2. **PostgreSQL Archive** - Long-term cold storage
   - Data older than 24-72 hours gets archived here
   - Used for historical analysis only

**Services Started by master.py:**
- FastAPI Data API (port 5050) - serves data to master2.py
- FastAPI Webhook Server (port 8001) - receives trade data
- PHP Web Server (port 8000) - serves website
- Binance Order Book Stream (WebSocket)
- Jupiter Price Fetcher (every 1 second)
- Data cleanup jobs (hourly)

**âš ï¸ NEVER add trading logic to master.py - it ONLY handles raw data**

---

### Master2.py - TRADING LOGIC (Port 5052)
**Purpose:** All trading computation and decision making - CAN restart independently

**Database Setup:**
1. **Local In-Memory DuckDB** - Completely separate instance
   - Master2 has its OWN in-memory DuckDB
   - Gets data FROM master.py via API at startup (backfill)
   - Runs all trading calculations on THIS local copy
   - Exposes computed data via FastAPI on port 5052

**How Data Flows:**
```
master.py (port 5050) 
  â†“ HTTP API backfill at startup
master2.py local DuckDB
  â†“ continuous sync via HTTP API
master2.py trading jobs
```

**Sync Strategy:**
- **Startup:** Loads 2 hours of historical data from master.py API
- **Runtime:** Fetches new data incrementally every 1-5 seconds
- **Computed Data:** Stays in master2's local DuckDB (cycles, profiles, patterns)

**Jobs in master2.py:**
- Price cycle analysis (every 2 seconds)
- Wallet profile building (every 10 seconds)
- Trade validation (train_validator)
- Trade following (follow_the_goat)
- Trailing stop monitoring
- Pattern creation

**âš ï¸ Master2 can restart without stopping master.py - data feeds continue**

---

## Database Access Patterns

### From master.py context:
```python
from core.database import get_trading_engine

engine = get_trading_engine()  # Gets master.py's in-memory DuckDB
engine.execute("INSERT INTO prices ...")
```

### From master2.py context (scheduler jobs):
```python
from core.data_client import get_client

client = get_client()  # Connects to master.py API on port 5050
prices = client.get_backfill("prices", hours=2)  # Fetch from master.py

# Then insert into master2's local DuckDB via write queue
from scheduler.master2 import queue_write, _local_duckdb
queue_write(_local_duckdb.execute, "INSERT INTO prices ...")
```

### From feature modules / trading modules (RECOMMENDED):
```python
from core.database import get_duckdb

# âœ… CORRECT: Use read_only=True for SELECT queries
with get_duckdb("central", read_only=True) as cursor:
    result = cursor.execute("SELECT * FROM prices WHERE token = 'SOL'").fetchall()

# âœ… CORRECT: Use duckdb_execute_write() for INSERT/UPDATE/DELETE
from core.database import duckdb_execute_write
duckdb_execute_write("central", "UPDATE table SET col = ? WHERE id = ?", [value, id])
```

---

## ğŸš¨ CRITICAL: Master2.py Database Read/Write Rules ğŸš¨

**Master2.py uses an in-memory DuckDB with a WRITE QUEUE for thread safety.**

### READ Operations (SELECT queries)

**ALWAYS use `read_only=True`** - this returns a fresh cursor that sees latest data:

```python
from core.database import get_duckdb

# âœ… CORRECT - Always use read_only=True for reads
with get_duckdb("central", read_only=True) as cursor:
    result = cursor.execute("SELECT * FROM prices ORDER BY id DESC LIMIT 1").fetchone()
    price = result[0] if result else None

# âŒ WRONG - Without read_only=True, you get the connection (meant for writes)
with get_duckdb("central") as conn:
    # This works but is slower and blocks other operations
    result = conn.execute("SELECT * FROM prices").fetchall()
```

### WRITE Operations (INSERT/UPDATE/DELETE)

**ALWAYS use `duckdb_execute_write()`** - this queues writes for thread safety:

```python
from core.database import duckdb_execute_write

# âœ… CORRECT - Fire-and-forget write (non-blocking)
duckdb_execute_write("central", 
    "INSERT INTO my_table (col1, col2) VALUES (?, ?)", 
    [value1, value2])

# âœ… CORRECT - Synchronous write (blocks until complete)
duckdb_execute_write("central", 
    "UPDATE my_table SET status = ? WHERE id = ?", 
    [new_status, record_id], 
    sync=True)

# âŒ WRONG - Direct writes bypass the queue and cause conflicts
with get_duckdb("central") as conn:
    conn.execute("INSERT INTO my_table ...")  # DON'T DO THIS
```

### Why This Architecture?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MASTER2.PY DATABASE ARCHITECTURE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   WRITE PATH (Serialized via Queue)                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚   duckdb_execute_write()                                            â”‚
â”‚         â†“                                                           â”‚
â”‚   Write Queue (thread-safe)                                         â”‚
â”‚         â†“                                                           â”‚
â”‚   Background Writer Thread                                          â”‚
â”‚         â†“                                                           â”‚
â”‚   _local_duckdb.execute() [with lock]                               â”‚
â”‚                                                                     â”‚
â”‚   READ PATH (Concurrent via Fresh Cursors)                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚   get_duckdb("central", read_only=True)                             â”‚
â”‚         â†“                                                           â”‚
â”‚   Creates FRESH cursor [with lock]                                  â”‚
â”‚         â†“                                                           â”‚
â”‚   cursor.execute() [no lock needed - concurrent reads OK]           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Points:**
1. Writes go through a queue â†’ serialized, no conflicts
2. Reads get fresh cursors â†’ always see latest data
3. Multiple reads can happen concurrently (no blocking)
4. Writes and reads don't block each other

### Common Patterns in Trading Modules

```python
# Pattern 1: Read current price
from core.database import get_duckdb

def get_current_sol_price():
    with get_duckdb("central", read_only=True) as cursor:
        result = cursor.execute("""
            SELECT price FROM prices 
            WHERE token = 'SOL' 
            ORDER BY id DESC LIMIT 1
        """).fetchone()
        return float(result[0]) if result else None

# Pattern 2: Update a record after processing
from core.database import duckdb_execute_write

def mark_trade_complete(trade_id, status):
    duckdb_execute_write("central", 
        "UPDATE follow_the_goat_buyins SET our_status = ? WHERE id = ?",
        [status, trade_id])

# Pattern 3: Insert new record with sync (when you need the ID immediately)
def insert_and_get_id():
    # First get the next ID
    with get_duckdb("central", read_only=True) as cursor:
        result = cursor.execute("SELECT MAX(id) + 1 FROM my_table").fetchone()
        next_id = result[0] if result and result[0] else 1
    
    # Then insert with sync=True to ensure it's committed
    duckdb_execute_write("central",
        "INSERT INTO my_table (id, data) VALUES (?, ?)",
        [next_id, "my_data"],
        sync=True)
    
    return next_id
```

### âŒ Anti-Patterns to Avoid

```python
# âŒ WRONG: Caching database connections or cursors
class BadService:
    def __init__(self):
        self.cursor = get_duckdb("central")  # DON'T cache connections!
    
    def query(self):
        return self.cursor.execute("SELECT ...").fetchall()  # Stale data!

# âŒ WRONG: Direct connection access for writes
with get_duckdb("central") as conn:
    conn.execute("INSERT INTO ...")  # Bypasses write queue!

# âŒ WRONG: Forgetting read_only=True
with get_duckdb("central") as conn:  # Missing read_only=True
    result = conn.execute("SELECT ...").fetchall()  # Works but inefficient
```

---

## Core Principles

### 1. Clean Code
- Minimal files - consolidate where possible
- Clear, descriptive naming
- Single responsibility per module
- No duplicate code across features

### 2. Database: DuckDB + PostgreSQL
- **DuckDB** - Hot storage (in-memory, fast access)
  - master.py: TradingDataEngine (raw data)
  - master2.py: Local instance (computed data + synced raw data)
- **PostgreSQL** - Cold storage (archive only)
  - Archived data older than 24-72 hours
  - Used for historical analysis
- All database setup is documented in `duckdb/ARCHITECTURE.md`


### 3. Hot/Cold Storage Pattern
```python
# Example: price_points (hot) + price_points_archive (cold)
# Data older than 24 hours is moved from hot to cold storage
```

### 4. Scheduling: APScheduler Only
- **NEVER use .bat files** for scheduling Python scripts
- Data ingestion jobs run in `scheduler/master.py`
- Trading logic jobs run in `scheduler/master2.py`
- Use APScheduler with cron or interval triggers

```python
# Correct: APScheduler in master.py for data jobs
scheduler.add_job(fetch_jupiter_prices, 'interval', seconds=1)

# Correct: APScheduler in master2.py for trading jobs
scheduler.add_job(follow_the_goat_job, 'interval', seconds=5)

# WRONG: Never create .bat files for scheduling
```

---

## Port Assignments

| Port | Service | Master | Purpose |
|------|---------|--------|---------|
| 5050 | FastAPI Data API | master.py | Serves raw data from TradingDataEngine |
| 5052 | FastAPI Local API | master2.py | Serves computed trading data |
| 8000 | PHP Server | master.py | Website frontend |
| 8001 | FastAPI Webhook | master.py | Receives trade data from QuickNode |

---

## Project Structure

```
follow_the_goat/
â”œâ”€â”€ .cursorrules           # This file - READ THIS FIRST
â”œâ”€â”€ core/                  # Shared utilities
â”‚   â”œâ”€â”€ database.py        # DuckDB/PostgreSQL connection manager
â”‚   â”œâ”€â”€ data_client.py     # HTTP client for master.py API (used by master2)
â”‚   â”œâ”€â”€ data_api.py        # FastAPI app for master.py (port 5050)
â”‚   â”œâ”€â”€ config.py          # Environment and settings
â”‚   â””â”€â”€ logging.py         # Centralized logging
â”œâ”€â”€ scheduler/             # APScheduler masters
â”‚   â”œâ”€â”€ master.py          # DATA ENGINE - raw data ingestion (port 5050)
â”‚   â”œâ”€â”€ master2.py         # TRADING LOGIC - all computations (port 5052)
â”‚   â””â”€â”€ status.py          # Job status tracking (shared)
â”œâ”€â”€ features/              # Individual feature modules
â”‚   â””â”€â”€ [feature_name]/    # One folder per feature
â”œâ”€â”€ duckdb/                # Database documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md    # Central DB schema reference
â”‚   â””â”€â”€ duckdb.exe         # DuckDB CLI
â””â”€â”€ 000old_code/           # Legacy code (DO NOT MODIFY, reference only)
```

---

## When to Use Which Master

### Add to master.py IF:
âœ… Fetching data from external API (Jupiter, Binance, etc.)
âœ… Receiving data via webhook
âœ… Data cleanup/archiving jobs
âœ… Raw data storage

### Add to master2.py IF:
âœ… Analyzing/processing existing data
âœ… Making trading decisions
âœ… Building wallet profiles
âœ… Pattern detection
âœ… Any computation on data

**Rule of Thumb:** If it creates NEW raw data, it goes in master.py. If it PROCESSES existing data, it goes in master2.py.

---

## Database Conventions

### Required Columns
Every table must have:
- `id`: INTEGER PRIMARY KEY (auto-increment)
- `created_at`: TIMESTAMP DEFAULT CURRENT_TIMESTAMP

### Indexes
Always create indexes on:
- Timestamp columns used in queries
- Foreign key relationships
- Columns used in WHERE clauses

---

## Code Patterns

### Accessing Data in master.py
```python
from core.database import get_trading_engine

# Direct access to master.py's in-memory DuckDB
engine = get_trading_engine()
engine.execute("INSERT INTO prices VALUES (?, ?, ?)", [ts, token, price])
results = engine.read_all("SELECT * FROM prices WHERE token = 'SOL'")
```

### Accessing Data in master2.py (scheduler jobs only)
```python
from core.data_client import get_client

# STEP 1: Fetch data from master.py via API
client = get_client()  # Connects to http://localhost:5050
prices = client.get_backfill("prices", hours=2)

# STEP 2: Insert into master2's local DuckDB via write queue
from scheduler.master2 import queue_write, _local_duckdb
queue_write(_local_duckdb.execute, 
    "INSERT INTO prices VALUES (?, ?, ?)", 
    [ts, token, price])
```

### Accessing Data from Feature/Trading Modules (RECOMMENDED)
```python
from core.database import get_duckdb, duckdb_execute_write

# âœ… READING: Always use read_only=True for SELECT queries
with get_duckdb("central", read_only=True) as cursor:
    result = cursor.execute("SELECT * FROM prices WHERE token = 'SOL'").fetchall()
    # cursor is a fresh cursor that sees latest data

# âœ… WRITING: Always use duckdb_execute_write for INSERT/UPDATE/DELETE
duckdb_execute_write("central", 
    "UPDATE follow_the_goat_buyins SET our_status = ? WHERE id = ?",
    [new_status, buyin_id])
```

### Feature Module Structure
```python
# features/my_feature/main.py
"""
Feature: My Feature Name
Migrated from: 000old_code/solana_node/path/to/original.py
Runs in: master2.py (trading logic)
"""

from core.database import get_duckdb, duckdb_execute_write

def run():
    """Main entry point for this feature."""
    # Feature runs in master2.py context
    
    # âœ… READING: Always use read_only=True
    with get_duckdb("central", read_only=True) as cursor:
        prices = cursor.execute(
            "SELECT * FROM prices WHERE token = 'SOL' ORDER BY ts DESC LIMIT 10"
        ).fetchall()
    
    # Process data...
    result = analyze_prices(prices)
    
    # âœ… WRITING: Always use duckdb_execute_write
    if result:
        duckdb_execute_write("central",
            "INSERT INTO my_results (data, created_at) VALUES (?, NOW())",
            [result])
```

### Scheduled Task Registration - master.py (Data Jobs)
```python
# In scheduler/master.py
from get_prices_from_jupiter import fetch_and_store_once

scheduler.add_job(
    fetch_and_store_once,
    trigger='interval',
    seconds=1,
    id='fetch_jupiter_prices',
    name='Fetch prices from Jupiter API',
    executor='realtime'
)
```

### Scheduled Task Registration - master2.py (Trading Jobs)
```python
# In scheduler/master2.py
from features.my_feature.main import run as my_feature_run

scheduler.add_job(
    my_feature_run,
    trigger='interval',
    seconds=10,
    id='my_feature',
    name='My Feature Description',
    executor='trading'
)
```

---

## Migration Workflow

When migrating a feature from `000old_code/`:

1. **Reference only** - read the old code but don't modify it
2. **Determine which master** - data ingestion (master.py) or trading logic (master2.py)?
3. **Create new module** in `features/[feature_name]/`
4. **Document origin** - note which old file(s) this replaces
5. **Update database access** - use correct pattern for master.py or master2.py context
6. **Implement hot/cold pattern** for any data storage
7. **Register with correct scheduler** - master.py for data jobs, master2.py for trading jobs
8. **Test thoroughly** before marking migration complete

---

## Forbidden Practices

### General
âŒ Never look in `/000old_code/` unless directly referenced for migration  
âŒ Never create `.bat` files for scheduling  
âŒ Never add trading logic to master.py (data ingestion only)  
âŒ Never add data ingestion to master2.py (trading logic only)  
âŒ Never access master.py's TradingDataEngine from master2.py directly (use API)  
âŒ Never create duplicate utility files - use `core/`  
âŒ Never skip the hot/cold storage pattern for time-series data  
âŒ Never forget that master2.py has its OWN separate in-memory DuckDB

### Database Access (CRITICAL)
âŒ Never cache DuckDB connections or cursors - always get fresh ones  
âŒ Never use `get_duckdb("central")` without `read_only=True` for SELECT queries  
âŒ Never write directly via `conn.execute("INSERT/UPDATE")` - use `duckdb_execute_write()`  
âŒ Never store a cursor in a class attribute or global variable (leads to stale data)  
âŒ Never assume data written via queue_write is immediately visible without read_only=True

---

## Critical Reminders for AI Agents

### ğŸ”´ MOST IMPORTANT: Two Separate Databases
- **master.py** has TradingDataEngine (in-memory DuckDB) + PostgreSQL archive
- **master2.py** has its OWN in-memory DuckDB (completely separate instance)
- They communicate via HTTP API (port 5050), NOT shared memory
- master2 syncs data FROM master.py at startup and continuously

### ğŸ”´ Which Master for What
- **Data ingestion** (APIs, webhooks, raw data) â†’ master.py
- **Data processing** (analysis, decisions, patterns) â†’ master2.py
- When in doubt: "Does this CREATE new raw data?" â†’ master.py, else â†’ master2.py

### ğŸ”´ Database Access Patterns
- In master.py: `get_trading_engine()` for direct DuckDB access
- In master2.py scheduler: `get_client()` to fetch from master.py, then `queue_write()` to local DuckDB
- In features/trading modules:
  - **READS:** `get_duckdb("central", read_only=True)` - ALWAYS use read_only=True for SELECT
  - **WRITES:** `duckdb_execute_write("central", sql, params)` - ALWAYS use this for INSERT/UPDATE/DELETE

### ğŸ”´ Database Do's and Don'ts
```python
# âœ… DO: Use read_only=True for all SELECT queries
with get_duckdb("central", read_only=True) as cursor:
    result = cursor.execute("SELECT * FROM prices").fetchall()

# âœ… DO: Use duckdb_execute_write for all writes
duckdb_execute_write("central", "UPDATE table SET x = ?", [value])

# âŒ DON'T: Cache connections or cursors (leads to stale data)
# âŒ DON'T: Use get_duckdb() without read_only=True for reads
# âŒ DON'T: Write directly via get_duckdb() - bypasses write queue
```

### ğŸ”´ Restart Independence
- master.py runs continuously (data feeds must never stop)
- master2.py can restart independently (trading logic updates don't affect data)
- Restarting master2.py triggers re-sync from master.py API

---

## File Locations

| Purpose | Location |
|---------|----------|
| DuckDB databases | In-memory only (no files) |
| PostgreSQL archive | See `core/config.py` for connection details |
| DuckDB CLI | `duckdb/duckdb.exe` |
| DB documentation | `duckdb/ARCHITECTURE.md` |
| Data Engine scheduler | `scheduler/master.py` |
| Trading Logic scheduler | `scheduler/master2.py` |
| Shared code | `core/` |
| Features | `features/` |
| Legacy reference | `000old_code/` (read-only) |

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MASTER.PY - DATA ENGINE                     â”‚
â”‚                    (Runs Indefinitely)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     TradingDataEngine (In-Memory DuckDB)                â”‚   â”‚
â”‚  â”‚     - Raw price data (SOL, BTC, ETH)                    â”‚   â”‚
â”‚  â”‚     - Order book snapshots (Binance)                    â”‚   â”‚
â”‚  â”‚     - Trade data (sol_stablecoin_trades)                â”‚   â”‚
â”‚  â”‚     - Hot storage only (24-72 hours)                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â–²                                      â”‚
â”‚                          â”‚ Insert Data                          â”‚
â”‚                          â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Jupiter API    â”‚ Binance WS   â”‚ QuickNode Webhook       â”‚   â”‚
â”‚  â”‚ (every 1s)     â”‚ (real-time)  â”‚ (FastAPI port 8001)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     FastAPI Data API (Port 5050)                        â”‚   â”‚
â”‚  â”‚     - /query - Execute SELECT queries                   â”‚   â”‚
â”‚  â”‚     - /backfill/{table} - Historical data for startup   â”‚   â”‚
â”‚  â”‚     - /sync/{table} - Incremental sync by ID            â”‚   â”‚
â”‚  â”‚     - /insert - Queue write to DuckDB                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â”‚ HTTP API                             â”‚
â”‚                          â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     PostgreSQL Archive (Cold Storage)                   â”‚   â”‚
â”‚  â”‚     - Data older than 24-72 hours                       â”‚   â”‚
â”‚  â”‚     - Historical analysis only                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Other Services:                                                â”‚
â”‚  - PHP Web Server (port 8000)                                   â”‚
â”‚  - Cleanup jobs (hourly)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ HTTP API (port 5050)
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MASTER2.PY - TRADING LOGIC                   â”‚
â”‚                    (Can Restart Independently)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     DataClient (HTTP Client)                            â”‚   â”‚
â”‚  â”‚     - Connects to master.py API (port 5050)             â”‚   â”‚
â”‚  â”‚     - Fetches 2 hours of data at startup                â”‚   â”‚
â”‚  â”‚     - Continuous sync every 1-5 seconds                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â”‚ Insert into Local DB                 â”‚
â”‚                          â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Local In-Memory DuckDB (SEPARATE INSTANCE)          â”‚   â”‚
â”‚  â”‚     - Synced raw data (prices, trades, order book)      â”‚   â”‚
â”‚  â”‚     - Computed data (cycles, profiles, patterns)        â”‚   â”‚
â”‚  â”‚     - Thread-safe with write queue + read cursors       â”‚   â”‚
â”‚  â”‚     - Registered as "central" for feature modules       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â”‚ Read Data                            â”‚
â”‚                          â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Trading Jobs (APScheduler)                          â”‚   â”‚
â”‚  â”‚     - Price cycle analysis (every 2s)                   â”‚   â”‚
â”‚  â”‚     - Wallet profile building (every 10s)               â”‚   â”‚
â”‚  â”‚     - Trade validation (train_validator)                â”‚   â”‚
â”‚  â”‚     - Trade following (follow_the_goat)                 â”‚   â”‚
â”‚  â”‚     - Trailing stop monitoring                          â”‚   â”‚
â”‚  â”‚     - Pattern detection                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     FastAPI Local API (Port 5052)                       â”‚   â”‚
â”‚  â”‚     - Serves computed trading data                      â”‚   â”‚
â”‚  â”‚     - /cycles - Price cycle analysis                    â”‚   â”‚
â”‚  â”‚     - /profiles - Wallet profiles                       â”‚   â”‚
â”‚  â”‚     - /buyins - Active positions                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

