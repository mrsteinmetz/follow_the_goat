# Follow The Goat - Project Rules

## Project Overview
This project uses a **PostgreSQL-only architecture** for all data storage and retrieval.

---

### DuckDB Exception for Filter Analysis

**IMPORTANT:** There is ONE exception to the PostgreSQL-only rule:

**`core/filter_cache.py`** is permitted to use DuckDB for read-only caching of filter analysis data.

This is a performance optimization that:
- Speeds up filter pattern generation by 10-50x
- Uses DuckDB purely as a read-only cache (7-day rolling window)
- PostgreSQL remains the source of truth
- Cache is automatically synced incrementally from PostgreSQL
- NO other modules should use DuckDB


## ğŸš¨ CRITICAL: DUAL-MASTER ARCHITECTURE ğŸš¨

**THIS SYSTEM USES TWO SEPARATE PROCESSES SHARING ONE DATABASE**

### Master.py - DATA ENGINE (Ports 8000, 8001)
**Purpose:** Raw data ingestion - runs continuously

**Database:** PostgreSQL (shared with master2.py)
- Writes: prices, trades, order book data
- Continuous operation without restarts

**Services Started by master.py:**
- FastAPI Webhook Server (port 8001) - receives trade data
- PHP Web Server (port 8000) - serves website
- Binance Order Book Stream (WebSocket)
- Jupiter Price Fetcher (every 1 second)
- Data cleanup jobs (hourly)

**âš ï¸ NEVER add trading logic to master.py - it ONLY handles raw data**

---

### Master2.py - TRADING LOGIC (Port 5052)
**Purpose:** All trading computation and decision making

**Database:** PostgreSQL (shared with master.py)
- Reads: prices, trades, cycles
- Writes: buyins, positions, patterns, profiles
- Can restart independently without data loss

**How Data Flows:**
```
master.py â†’ PostgreSQL â† master2.py
              â†‘
         website_api.py (port 5051)
```

**Jobs in master2.py:**
- Trade validation (train_validator)
- Trade following (follow_the_goat) 
- Trailing stop monitoring
- Potential gains updates
- Pattern creation

**âœ… Master2 can restart freely - all data persists in PostgreSQL**

---

## Database: PostgreSQL Only

### Single Source of Truth
- **One PostgreSQL database** for everything
- No hot/cold storage split
- No in-memory caching
- No data syncing between processes

### Connection Management
```python
from core.database import get_postgres, postgres_execute, postgres_query

# Read data
with get_postgres() as conn:
    with conn.cursor() as cursor:
        cursor.execute("SELECT * FROM prices WHERE token = %s", ['SOL'])
        results = cursor.fetchall()  # Returns list of dicts

# Write data
postgres_execute(
    "INSERT INTO prices (timestamp, token, price) VALUES (NOW(), %s, %s)",
    ['SOL', 123.45]
)
```

### Connection Pooling
- `psycopg2.pool.SimpleConnectionPool` (5-20 connections)
- Thread-safe
- Automatic connection management via context managers
- No manual connection cleanup needed

---

## Core Principles

### 1. Clean Code
- Minimal files - consolidate where possible
- Clear, descriptive naming
- Single responsibility per module
- No duplicate code across features

### 2. Database Access Patterns

**Reading Data:**
```python
from core.database import get_postgres

with get_postgres() as conn:
    with conn.cursor() as cursor:
        cursor.execute("""
            SELECT price, timestamp FROM prices 
            WHERE token = %s 
            ORDER BY timestamp DESC LIMIT 1
        """, ['SOL'])
        result = cursor.fetchone()
```

**Writing Data:**
```python
from core.database import postgres_execute

rows_affected = postgres_execute(
    "UPDATE follow_the_goat_buyins SET our_status = %s WHERE id = %s",
    ['sold', 123]
)
```

**Bulk Insert:**
```python
from core.database import postgres_insert_many

postgres_insert_many("prices", [
    {"timestamp": ts1, "token": "SOL", "price": 123.45},
    {"timestamp": ts2, "token": "BTC", "price": 50000.00},
])
```

### 3. Important SQL Syntax Differences

**Parameter Placeholders:**
```python
# âœ… CORRECT (PostgreSQL)
cursor.execute("SELECT * FROM table WHERE id = %s", [123])

# âŒ WRONG (DuckDB syntax - don't use)
cursor.execute("SELECT * FROM table WHERE id = ?", [123])
```

**Upsert:**
```python
# PostgreSQL
cursor.execute("""
    INSERT INTO table (id, val) VALUES (%s, %s)
    ON CONFLICT (id) DO UPDATE SET val = EXCLUDED.val
""", [1, 'x'])

cursor.execute("""
    INSERT INTO table (id, val) VALUES (%s, %s)
    ON CONFLICT DO NOTHING
""", [1, 'x'])
```

**Data Types:**
- Use `BIGSERIAL` for auto-increment primary keys
- Use `DOUBLE PRECISION` for decimals
- Use `JSONB` for JSON columns
- Use `SMALLINT` for small integers
- Use `BOOLEAN` for flags
- Use `INTERVAL '24 hours'` for time arithmetic

### 4. Scheduling: APScheduler Only
- **NEVER use .bat files** for scheduling Python scripts
- Data ingestion jobs run in `scheduler/master.py`
- Trading logic jobs run in `scheduler/master2.py`
- Use APScheduler with cron or interval triggers

```python
# Correct: APScheduler in master.py for data jobs
scheduler.add_job(fetch_jupiter_prices, 'interval', seconds=1)

# Correct: APScheduler in master2.py for trading jobs
scheduler.add_job(follow_the_goat_job, 'interval', seconds=1)
```

---

## Port Assignments

| Port | Service | Master | Purpose |
|------|---------|--------|---------|
| 5051 | Flask Website API | website_api.py | Serves website data from PostgreSQL |
| 5052 | FastAPI Local API | master2.py | Trading logic queries |
| 8000 | PHP Server | master.py | Website frontend |
| 8001 | FastAPI Webhook | master.py | Receives trade data from QuickNode |

---

## Project Structure

```
follow_the_goat/
â”œâ”€â”€ .cursorrules # This file - READ THIS FIRST
â”œâ”€â”€ core/ # Shared utilities
â”‚ â”œâ”€â”€ database.py # PostgreSQL connection manager
â”‚ â”œâ”€â”€ config.py # Environment and settings
â”‚ â””â”€â”€ logging.py # Centralized logging
â”œâ”€â”€ scheduler/ # APScheduler masters
â”‚ â”œâ”€â”€ master.py # DATA ENGINE - raw data ingestion
â”‚ â”œâ”€â”€ master2.py # TRADING LOGIC - all computations
â”‚ â”œâ”€â”€ website_api.py # Website API (Flask, port 5051)
â”‚ â””â”€â”€ status.py # Job status tracking (shared)
â”œâ”€â”€ features/ # Individual feature modules
â”‚ â””â”€â”€ [feature_name]/ # One folder per feature
â”œâ”€â”€ scripts/ # Database migration scripts
â”‚ â””â”€â”€ postgres_schema.sql # Complete PostgreSQL schema
â””â”€â”€ 000trading/ # Trading modules
```

---

## When to Use Which Master

### Add to master.py IF:
âœ… Fetching data from external API (Jupiter, Binance, etc.)
âœ… Receiving data via webhook
âœ… Data cleanup/archiving jobs
âœ… Raw data storage

### Add to master2.py IF:
âœ… Analyzing/processing existing data
âœ… Making trading decisions
âœ… Building wallet profiles
âœ… Pattern detection
âœ… Any computation on data

**Rule of Thumb:** If it creates NEW raw data, it goes in master.py. If it PROCESSES existing data, it goes in master2.py.

---

## Database Conventions

### Required Columns
Every table should have:
- `id`: BIGSERIAL PRIMARY KEY (or SERIAL for smaller tables)
- `created_at` or `timestamp`: TIMESTAMP DEFAULT CURRENT_TIMESTAMP

### Indexes
Always create indexes on:
- Timestamp columns used in queries
- Foreign key relationships
- Columns used in WHERE clauses

---

## Code Patterns

### Feature Module Structure
```python
# features/my_feature/main.py
"""
Feature: My Feature Name
Runs in: master2.py (trading logic)
"""

from core.database import get_postgres, postgres_execute

def run():
    """Main entry point for this feature."""
    
    # Read data
    with get_postgres() as conn:
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT * FROM prices 
                WHERE token = %s 
                ORDER BY timestamp DESC LIMIT 10
            """, ['SOL'])
            prices = cursor.fetchall()
    
    # Process data...
    result = analyze_prices(prices)
    
    # Write result
    if result:
        postgres_execute("""
            INSERT INTO my_results (data, created_at) 
            VALUES (%s, NOW())
        """, [result])
```

### Scheduled Task Registration - master.py (Data Jobs)
```python
# In scheduler/master.py
from get_prices_from_jupiter import fetch_and_store_once

scheduler.add_job(
    fetch_and_store_once,
    trigger='interval',
    seconds=1,
    id='fetch_jupiter_prices',
    name='Fetch prices from Jupiter API',
    executor='realtime'
)
```

### Scheduled Task Registration - master2.py (Trading Jobs)
```python
# In scheduler/master2.py

@track_job("my_feature", "My Feature Description")
def my_feature_job():
    """Job wrapper for my feature."""
    try:
        from features.my_feature.main import run
        run()
    except Exception as e:
        logger.error(f"My feature error: {e}", exc_info=True)

scheduler.add_job(
    my_feature_job,
    trigger='interval',
    seconds=10,
    id='my_feature',
    name='My Feature',
    executor='realtime'
)
```

---

## Migration Status

### âœ… Completed
- PostgreSQL schema created (`scripts/postgres_schema.sql`)
- `core/database.py` - PostgreSQL-only with connection pooling
- `scheduler/master.py` - Direct PostgreSQL writes
- `scheduler/master2.py` - Simplified trading logic (no backfill)
- `scheduler/website_api.py` - Direct PostgreSQL queries
- All trading modules - Updated to PostgreSQL syntax
- All data feed modules - Updated to PostgreSQL syntax

### ğŸ“‹ Architecture Benefits
- **Simpler:** One database, no syncing
- **Faster:** No backfill on restart (3s vs 2h)
- **Persistent:** All data survives restarts
- **Standard:** Industry-standard PostgreSQL tools

---

## Forbidden Practices

### General
âŒ Never use `.bat` files for scheduling
âŒ Never add trading logic to master.py (data ingestion only)
âŒ Never add data ingestion to master2.py (trading logic only)
âŒ Never create duplicate utility files - use `core/`
âŒ Never import `duckdb` - use PostgreSQL only

### Database Access (CRITICAL)
âŒ Never use `?` placeholders - use `%s` in PostgreSQL
âŒ Never use DuckDB syntax (INSERT OR REPLACE, etc.)
âŒ Never cache connections - always use context managers
âŒ Never use string formatting for SQL - use parameterized queries
âŒ Never forget `WITH get_postgres() as conn:` context manager

---

## Critical Reminders for AI Agents

### ğŸ”´ MOST IMPORTANT: PostgreSQL Only
- **ONE PostgreSQL database** shared by all processes
- NO in-memory databases
- NO data syncing or backfill logic
- master.py and master2.py both connect to the same PostgreSQL instance

### ğŸ”´ SQL Syntax
```python
# âœ… CORRECT (PostgreSQL)
cursor.execute("SELECT * FROM table WHERE id = %s", [123])

# âŒ WRONG (DuckDB - don't use)
cursor.execute("SELECT * FROM table WHERE id = ?", [123])
```

### ğŸ”´ Database Access Pattern
```python
# âœ… CORRECT
from core.database import get_postgres, postgres_execute

# Read
with get_postgres() as conn:
    with conn.cursor() as cursor:
        cursor.execute("SELECT ...")
        results = cursor.fetchall()

# Write
postgres_execute("UPDATE ...", [params])
```

### ğŸ”´ Restart Independence
- master.py runs continuously (data feeds must never stop)
- master2.py can restart independently (no data loss)
- website_api.py can restart any time
- All data persists in PostgreSQL

---

## File Locations

| Purpose | Location |
|---------|----------|
| PostgreSQL schema | `scripts/postgres_schema.sql` |
| PostgreSQL config | See `core/config.py` for connection details |
| Data Engine scheduler | `scheduler/master.py` |
| Trading Logic scheduler | `scheduler/master2.py` |
| Website API | `scheduler/website_api.py` |
| Shared code | `core/` |
| Features | `features/` |
| Trading modules | `000trading/` |
| Data feeds | `000data_feeds/` |

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MASTER.PY - DATA ENGINE â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”‚
â”‚ Services: â”‚
â”‚ - Jupiter Price Fetcher (every 1s) â”‚
â”‚ - Binance Order Book Stream (WebSocket) â”‚
â”‚ - QuickNode Webhook Server (port 8001) â”‚
â”‚ - PHP Web Server (port 8000) â”‚
â”‚ - Data Cleanup Jobs (hourly) â”‚
â”‚ â”‚
â”‚ Writes: prices, trades, order_book_features â”‚
â”‚ â”‚
â”‚ â†“ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POSTGRESQL DATABASE (Shared) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”‚
â”‚ Tables: â”‚
â”‚ - prices, sol_stablecoin_trades, order_book_features â”‚
â”‚ - cycle_tracker, wallet_profiles â”‚
â”‚ - follow_the_goat_plays, follow_the_goat_buyins â”‚
â”‚ - pattern_config_filters, filter_combinations â”‚
â”‚ - And 12 more tables... â”‚
â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†‘                           â†‘
           â”‚                           â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                  â”‚     â”‚                     â”‚
â”Œâ”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”   â”Œâ”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”
â”‚ MASTER2.PY        â”‚   â”‚ WEBSITE_API.PY        â”‚
â”‚ (Trading Logic)   â”‚   â”‚ (Flask API)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   â”‚   â”‚                       â”‚
â”‚ Jobs:             â”‚   â”‚ Endpoints:            â”‚
â”‚ - follow_the_goat â”‚   â”‚ - /health             â”‚
â”‚ - trailing_stop   â”‚   â”‚ - /cycles             â”‚
â”‚ - train_validator â”‚   â”‚ - /buyins             â”‚
â”‚ - update_gains    â”‚   â”‚ - /plays              â”‚
â”‚ - create_patterns â”‚   â”‚ - /profiles           â”‚
â”‚                   â”‚   â”‚ - /patterns           â”‚
â”‚ Local API: 5052   â”‚   â”‚                       â”‚
â”‚                   â”‚   â”‚ Port: 5051            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

End of .cursorrules
